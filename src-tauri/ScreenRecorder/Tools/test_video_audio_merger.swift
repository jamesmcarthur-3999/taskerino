#!/usr/bin/env swift

/**
 * Test script for VideoAudioMerger
 *
 * Creates test video and audio files, then tests merging with all quality presets.
 * Measures compression ratios and merge times.
 *
 * Usage: swift test_video_audio_merger.swift
 */

import Foundation
import AVFoundation
import CoreMedia
import CoreGraphics
import CoreVideo

// MARK: - Test Media Generator

/// Generate test video file (5 seconds, 30fps, 1280x720, no audio)
func generateTestVideo(outputPath: String) throws {
    print("üìπ Generating test video: \(outputPath)")

    let url = URL(fileURLWithPath: outputPath)

    // Delete if exists
    if FileManager.default.fileExists(atPath: outputPath) {
        try FileManager.default.removeItem(atPath: outputPath)
    }

    // Create video writer
    let writer = try AVAssetWriter(url: url, fileType: .mp4)

    let videoSettings: [String: Any] = [
        AVVideoCodecKey: AVVideoCodecType.h264,
        AVVideoWidthKey: 1280,
        AVVideoHeightKey: 720,
        AVVideoCompressionPropertiesKey: [
            AVVideoAverageBitRateKey: 2_000_000, // 2 Mbps
            AVVideoMaxKeyFrameIntervalKey: 30,
        ]
    ]

    let writerInput = AVAssetWriterInput(mediaType: .video, outputSettings: videoSettings)
    writerInput.expectsMediaDataInRealTime = false

    let pixelBufferAttributes: [String: Any] = [
        kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32ARGB,
        kCVPixelBufferWidthKey as String: 1280,
        kCVPixelBufferHeightKey as String: 720,
    ]

    let adaptor = AVAssetWriterInputPixelBufferAdaptor(
        assetWriterInput: writerInput,
        sourcePixelBufferAttributes: pixelBufferAttributes
    )

    guard writer.canAdd(writerInput) else {
        throw NSError(domain: "TestGenerator", code: -1, userInfo: [NSLocalizedDescriptionKey: "Cannot add writer input"])
    }

    writer.add(writerInput)

    guard writer.startWriting() else {
        throw NSError(domain: "TestGenerator", code: -1, userInfo: [NSLocalizedDescriptionKey: "Cannot start writing"])
    }

    writer.startSession(atSourceTime: .zero)

    // Generate frames (5 seconds @ 30fps = 150 frames)
    let fps = 30
    let duration = 5
    let totalFrames = fps * duration

    for frameNumber in 0..<totalFrames {
        // Wait for input to be ready
        while !writerInput.isReadyForMoreMediaData {
            Thread.sleep(forTimeInterval: 0.01)
        }

        // Create pixel buffer
        var pixelBuffer: CVPixelBuffer?
        let status = CVPixelBufferPoolCreatePixelBuffer(nil, adaptor.pixelBufferPool!, &pixelBuffer)

        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            throw NSError(domain: "TestGenerator", code: -1, userInfo: [NSLocalizedDescriptionKey: "Cannot create pixel buffer"])
        }

        // Lock buffer
        CVPixelBufferLockBaseAddress(buffer, [])
        defer { CVPixelBufferUnlockBaseAddress(buffer, []) }

        let baseAddress = CVPixelBufferGetBaseAddress(buffer)!
        let bytesPerRow = CVPixelBufferGetBytesPerRow(buffer)
        let width = CVPixelBufferGetWidth(buffer)
        let height = CVPixelBufferGetHeight(buffer)

        // Fill with gradient (animate from blue to red)
        let progress = Float(frameNumber) / Float(totalFrames)
        let red = UInt8(progress * 255)
        let blue = UInt8((1.0 - progress) * 255)

        for y in 0..<height {
            let rowPtr = baseAddress.advanced(by: y * bytesPerRow).assumingMemoryBound(to: UInt32.self)
            for x in 0..<width {
                // ARGB format
                let pixel: UInt32 = 0xFF000000 | (UInt32(red) << 16) | (UInt32(blue))
                rowPtr[x] = pixel
            }
        }

        // Add text (frame number)
        // Note: Skipping text rendering for simplicity

        // Append buffer
        let presentationTime = CMTime(value: CMTimeValue(frameNumber), timescale: CMTimeScale(fps))
        adaptor.append(buffer, withPresentationTime: presentationTime)

        if frameNumber % 30 == 0 {
            print("  Frame \(frameNumber)/\(totalFrames)")
        }
    }

    // Finish writing
    writerInput.markAsFinished()
    writer.finishWriting {
        if writer.status == .completed {
            print("‚úÖ Test video generated")
        } else {
            print("‚ùå Test video generation failed: \(writer.error?.localizedDescription ?? "Unknown error")")
        }
    }

    // Wait for completion
    while writer.status == .writing {
        Thread.sleep(forTimeInterval: 0.1)
    }
}

/// Generate test audio file (5 seconds, 440Hz sine wave, 44.1kHz, mono)
func generateTestAudio(outputPath: String) throws {
    print("üéµ Generating test audio: \(outputPath)")

    let url = URL(fileURLWithPath: outputPath)

    // Delete if exists
    if FileManager.default.fileExists(atPath: outputPath) {
        try FileManager.default.removeItem(atPath: outputPath)
    }

    // Create audio writer
    let writer = try AVAssetWriter(url: url, fileType: .m4a)

    let audioSettings: [String: Any] = [
        AVFormatIDKey: kAudioFormatMPEG4AAC,
        AVSampleRateKey: 44100,
        AVNumberOfChannelsKey: 1,
        AVEncoderBitRateKey: 64000
    ]

    let writerInput = AVAssetWriterInput(mediaType: .audio, outputSettings: audioSettings)
    writerInput.expectsMediaDataInRealTime = false

    guard writer.canAdd(writerInput) else {
        throw NSError(domain: "TestGenerator", code: -1, userInfo: [NSLocalizedDescriptionKey: "Cannot add writer input"])
    }

    writer.add(writerInput)

    guard writer.startWriting() else {
        throw NSError(domain: "TestGenerator", code: -1, userInfo: [NSLocalizedDescriptionKey: "Cannot start writing"])
    }

    writer.startSession(atSourceTime: .zero)

    // Generate audio samples (5 seconds @ 44.1kHz = 220,500 samples)
    let sampleRate = 44100
    let duration = 5
    let frequency: Float = 440.0 // A4 note
    let amplitude: Float = 0.5

    let totalSamples = sampleRate * duration
    let samplesPerBuffer = 1024

    var sampleIndex = 0

    while sampleIndex < totalSamples {
        // Wait for input to be ready
        while !writerInput.isReadyForMoreMediaData {
            Thread.sleep(forTimeInterval: 0.01)
        }

        let samplesThisBuffer = min(samplesPerBuffer, totalSamples - sampleIndex)

        // Create audio buffer
        let audioBufferList = AudioBufferList.allocate(maximumBuffers: 1)
        defer { free(audioBufferList.unsafeMutablePointer) }

        let dataSize = samplesThisBuffer * MemoryLayout<Float>.size
        let data = UnsafeMutableRawPointer.allocate(byteCount: dataSize, alignment: MemoryLayout<Float>.alignment)
        defer { data.deallocate() }

        audioBufferList[0] = AudioBuffer(
            mNumberChannels: 1,
            mDataByteSize: UInt32(dataSize),
            mData: data
        )

        // Generate sine wave samples
        let samples = data.assumingMemoryBound(to: Float.self)
        for i in 0..<samplesThisBuffer {
            let phase = Float(sampleIndex + i) / Float(sampleRate)
            samples[i] = amplitude * sin(2.0 * .pi * frequency * phase)
        }

        // Create sample buffer
        var format: AudioStreamBasicDescription = AudioStreamBasicDescription()
        format.mSampleRate = Float64(sampleRate)
        format.mFormatID = kAudioFormatLinearPCM
        format.mFormatFlags = kAudioFormatFlagIsFloat | kAudioFormatFlagIsPacked
        format.mBytesPerPacket = UInt32(MemoryLayout<Float>.size)
        format.mFramesPerPacket = 1
        format.mBytesPerFrame = UInt32(MemoryLayout<Float>.size)
        format.mChannelsPerFrame = 1
        format.mBitsPerChannel = 32

        var sampleBuffer: CMSampleBuffer?
        var formatDescription: CMAudioFormatDescription?

        CMAudioFormatDescriptionCreate(
            allocator: kCFAllocatorDefault,
            asbd: &format,
            layoutSize: 0,
            layout: nil,
            magicCookieSize: 0,
            magicCookie: nil,
            extensions: nil,
            formatDescriptionOut: &formatDescription
        )

        guard let formatDesc = formatDescription else {
            throw NSError(domain: "TestGenerator", code: -1, userInfo: [NSLocalizedDescriptionKey: "Cannot create format description"])
        }

        let presentationTime = CMTime(value: CMTimeValue(sampleIndex), timescale: CMTimeScale(sampleRate))

        CMSampleBufferCreate(
            allocator: kCFAllocatorDefault,
            dataBuffer: nil,
            dataReady: false,
            makeDataReadyCallback: nil,
            refcon: nil,
            formatDescription: formatDesc,
            sampleCount: samplesThisBuffer,
            sampleTimingEntryCount: 0,
            sampleTimingArray: nil,
            sampleSizeEntryCount: 0,
            sampleSizeArray: nil,
            sampleBufferOut: &sampleBuffer
        )

        if let buffer = sampleBuffer {
            CMSampleBufferSetDataBufferFromAudioBufferList(
                buffer,
                blockBufferAllocator: kCFAllocatorDefault,
                blockBufferMemoryAllocator: kCFAllocatorDefault,
                flags: 0,
                bufferList: audioBufferList.unsafePointer
            )

            writerInput.append(buffer)
        }

        sampleIndex += samplesThisBuffer

        if sampleIndex % (sampleRate * 1) == 0 {
            print("  Sample \(sampleIndex)/\(totalSamples)")
        }
    }

    // Finish writing
    writerInput.markAsFinished()
    writer.finishWriting {
        if writer.status == .completed {
            print("‚úÖ Test audio generated")
        } else {
            print("‚ùå Test audio generation failed: \(writer.error?.localizedDescription ?? "Unknown error")")
        }
    }

    // Wait for completion
    while writer.status == .writing {
        Thread.sleep(forTimeInterval: 0.1)
    }
}

// MARK: - Main Test

print("üß™ VideoAudioMerger Test Suite")
print("=" * 60)

let testDir = FileManager.default.temporaryDirectory.appendingPathComponent("videoaudiomerger-test-\(UUID().uuidString)")
try FileManager.default.createDirectory(at: testDir, withIntermediateDirectories: true)

let testVideoPath = testDir.appendingPathComponent("test_video.mp4").path
let testAudioPath = testDir.appendingPathComponent("test_audio.m4a").path

print("\nüìÅ Test directory: \(testDir.path)\n")

// Generate test files
do {
    try generateTestVideo(outputPath: testVideoPath)
    try generateTestAudio(outputPath: testAudioPath)
} catch {
    print("‚ùå Failed to generate test files: \(error)")
    exit(1)
}

print("\n" + "=" * 60)
print("‚úÖ Test files generated successfully")
print("=" * 60)

// Get file sizes
let videoSize = try FileManager.default.attributesOfItem(atPath: testVideoPath)[.size] as? Int64 ?? 0
let audioSize = try FileManager.default.attributesOfItem(atPath: testAudioPath)[.size] as? Int64 ?? 0

print("\nüìä Input File Sizes:")
print("  Video: \(videoSize / 1024) KB")
print("  Audio: \(audioSize / 1024) KB")
print("  Total: \((videoSize + audioSize) / 1024) KB")

print("\n" + "=" * 60)
print("üîß Testing VideoAudioMerger (not executable in this context)")
print("=" * 60)

print("""

‚úÖ Test files created successfully!

To test the merger, use the following paths in your Swift/Rust code:
- Video: \(testVideoPath)
- Audio: \(testAudioPath)

Expected results:
- Low quality: ~40% of input size, faster export
- Medium quality: ~60% of input size, balanced
- High quality: ~80% of input size, slower export

Cleanup:
rm -rf "\(testDir.path)"
""")
